{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.10 |Anaconda, Inc.| (default, May  7 2020, 19:46:08) [MSC v.1916 64 bit (AMD64)]\n",
      "1.19.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.summary API due to missing TensorBoard installation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.2\n",
      "2.3.0-tf\n",
      "2.3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"  # Force TF to use only the CPU\n",
    "\n",
    "import sys\n",
    "print(sys.version)\n",
    "\n",
    "import numpy as np\n",
    "print(np.__version__)\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "print(tf.keras.__version__)\n",
    "\n",
    "# assert tf.test.is_gpu_available() # raises error if gpu is not available\n",
    "# assert tf.test.is_built_with_cuda()\n",
    "# print(\"tf.test.is_built_with_cuda():\", tf.test.is_built_with_cuda())\n",
    "# print(\"list_physical_devices('GPU'):\", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# if tf.test.gpu_device_name():\n",
    "#     print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "# else:\n",
    "#     print(\"GPU installation is not available for TF\")\n",
    "\n",
    "import keras\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras functions\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from keras import layers\n",
    "from keras import models\n",
    "\n",
    "from keras.utils.vis_utils import plot_model\n",
    "# from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# instantiating a small convnet\n",
    "- a convnet takes as input tensors of shape (image_height, image_width, image_channels) (not including the batch dimension)\n",
    "- In this case, we’ll configure the convnet to process inputs of size (28, 28, 1), which is the format of MNIST images.\n",
    "- We’ll do this by passing the argument input_shape=(28, 28, 1) to the first layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 3, 3, 64)          36928     \n",
      "=================================================================\n",
      "Total params: 55,744\n",
      "Trainable params: 55,744\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 3, 3, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                36928     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 93,322\n",
      "Trainable params: 93,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# adding a classifier on top of the convnet\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "\n",
    "# 10-way classification, using a final layer with 10 outputs and a softmax activation\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "# (3, 3, 64) outputs are flattened into vectors of shape (576,) before going through two Dense layers.\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training DL models on MNIST images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
       "         18,  18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170,\n",
       "        253, 253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253,\n",
       "        253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253,\n",
       "        253, 198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253,\n",
       "        205,  11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,\n",
       "         90,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253,\n",
       "        190,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190,\n",
       "        253,  70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
       "        241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39,\n",
       "        148, 229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221,\n",
       "        253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253,\n",
       "        253, 253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253,\n",
       "        195,  80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,\n",
       "         11,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]], dtype=uint8)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.amax(train_images[0]) # the values ranges between 0-255 (i.e. grayscale image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "digit = train_images[1250]\n",
    "\n",
    "plt.imshow(digit, cmap=plt.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "digit = train_images[4]\n",
    "\n",
    "plt.imshow(digit, cmap=plt.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## densely connected network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_v0 = train_images.reshape((60000, 28*28)) # flatten 2D 28x28 images to 784x1 array\n",
    "train_images_v0 = train_images_v0.astype('float32') / 255 # scale the values in 0-1 interval\n",
    "\n",
    "test_images_v0 = test_images.reshape((10000, 28*28))\n",
    "test_images_v0 = test_images_v0.astype('float32') / 255\n",
    "\n",
    "train_labels_v0 = to_categorical(train_labels)\n",
    "test_labels_v0 = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(train_labels[0])\n",
    "print(train_labels_v0[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_labels:\t (60000,)\n",
      "train_labels_v0:\t (60000, 10)\n",
      "train_images:\t (60000, 28, 28)\n",
      "train_images_v0:\t (60000, 784)\n",
      "train_images[0]:\t (28, 28)\n",
      "train_images_v0[0]:\t (784,)\n"
     ]
    }
   ],
   "source": [
    "print(\"train_labels:\\t\", train_labels.shape)\n",
    "print(\"train_labels_v0:\\t\", train_labels_v0.shape)\n",
    "print(\"train_images:\\t\", train_images.shape)\n",
    "print(\"train_images_v0:\\t\", train_images_v0.shape)\n",
    "print(\"train_images[0]:\\t\", train_images[0].shape)\n",
    "print(\"train_images_v0[0]:\\t\", train_images_v0[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(train_images))\n",
    "print(type(train_images_v0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 3s 47us/step - loss: 0.2542 - accuracy: 0.9262\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 3s 45us/step - loss: 0.1038 - accuracy: 0.9696\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 3s 45us/step - loss: 0.0692 - accuracy: 0.9791\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 3s 46us/step - loss: 0.0495 - accuracy: 0.9850\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 3s 46us/step - loss: 0.0370 - accuracy: 0.9891\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1ef30e07e10>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_images_v0, train_labels_v0, epochs=5, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 31us/step\n",
      "test_loss: 0.069, test_acc: 0.979\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images_v0, test_labels_v0)\n",
    "print(\"test_loss: %s, test_acc: %s\" %(np.round(test_loss,3), np.round(test_acc,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_v1 = train_images.reshape((60000, 28, 28, 1)) # convert 2D 28x28 images to (28x28x1) tensors\n",
    "train_images_v1 = train_images_v1.astype('float32') / 255 # scale the values in 0-1 interval\n",
    "\n",
    "test_images_v1 = test_images.reshape((10000, 28, 28, 1))\n",
    "test_images_v1 = test_images_v1.astype('float32') / 255\n",
    "\n",
    "train_labels_v1 = to_categorical(train_labels)\n",
    "test_labels_v1 = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(np.max(train_images[0]))\n",
    "print(np.max(train_images_v1[0])) # the values ranges between 0-1 (i.e. normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_labels:\t (60000,)\n",
      "train_labels_v1:\t (60000, 10)\n",
      "train_images:\t (60000, 28, 28)\n",
      "train_images_v1:\t (60000, 28, 28, 1)\n",
      "train_images[0]:\t (28, 28)\n",
      "train_images_v1[0]:\t (28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"train_labels:\\t\", train_labels.shape)\n",
    "print(\"train_labels_v1:\\t\", train_labels_v1.shape)\n",
    "print(\"train_images:\\t\", train_images.shape)\n",
    "print(\"train_images_v1:\\t\", train_images_v1.shape)\n",
    "print(\"train_images[0]:\\t\", train_images[0].shape)\n",
    "print(\"train_images_v1[0]:\\t\", train_images_v1[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(train_images))\n",
    "print(type(train_images_v1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "digit = train_images_v1[1250][:,:,0]\n",
    "\n",
    "plt.imshow(digit, cmap=plt.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## building convnet model\n",
    "- a convnet takes as input tensors of shape *(image_height, image_width, image_channels)* -- not including the batch dimension\n",
    "- configure the convnet to process inputs of size (28, 28, 1), which is the format of MNIST images\n",
    "    - do this by passing the argument input_shape=(28, 28, 1) to the first layer\n",
    "- output of every Conv2D and MaxPooling2D layer is a 3D tensor of shape (height, width, channels).\n",
    "- the width and height dimensions tend to shrink as you go deeper in the network\n",
    "- The number of channels is controlled by the first argument passed to the Conv2D layers (32 or 64)\n",
    "- Adding a classifier on top of the convnet:\n",
    "    - last output tensor of shape (3,3,64) are fed into a densely connected classifier network (i.e. to a stack of dense layers)\n",
    "    - first 3D outputs need to be flattened into 1D, and then add a few dense layers on top\n",
    "- Densely connected layer vs convolution layer:\n",
    "    - Dense layers learn global patterns in their input feature space, whereas convolution layers learn local patterns\n",
    "        - the patterns learned by convolution layers are translation invariant\n",
    "        - convolution layers can learn spatial hierarchies of patterns: a first convolution layer will learn small local patterns such as edges, a second convolution layer will learn larger patterns made of the features of the first layers and son on. This allows convnets to efficiently learn increasingly complex and abstract visual concepts (because visual world is fundamentally spatially hierarchical)\n",
    "- Convolutions operate over 3D tensors, called feature maps, with two spatial axes (height and width) as well as a depth axis (also called the channels axis)\n",
    "    - For an RGB image, the dimension of the depth axis is 3, because the image has three color channels: red, green, and blue. For a black-and-white picture, like the MNIST digits, the depth is 1 (levels of gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_19 (Conv2D)           (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 3, 3, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 64)                36928     \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 93,322\n",
      "Trainable params: 93,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "\n",
    "# Conv2D(output_depth, (window_height, window_width))\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "\n",
    "# model.add(layers.Conv2D(32, (5, 5), activation='relu', input_shape=(28, 28, 1)))\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "# model.add(layers.Conv2D(64, (5, 5), activation='relu'))\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "# model.add(layers.Conv2D(64, (3, 3), activation='relu')) # if you put (5,5) conv here, it raises error\n",
    "\n",
    "# adding a classifier on top of the convnet\n",
    "# (3, 3, 64) outputs are flattened into vectors of shape (576,) before going through two Dense layers.\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "\n",
    "# 10-way classification, using a final layer with 10 outputs and a softmax activation\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop', \n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 23s 386us/step - loss: 0.1680 - accuracy: 0.9465\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 23s 379us/step - loss: 0.0437 - accuracy: 0.9867\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 22s 363us/step - loss: 0.0300 - accuracy: 0.9910\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 22s 363us/step - loss: 0.0229 - accuracy: 0.9931\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 21s 356us/step - loss: 0.0194 - accuracy: 0.9939\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1ef3177e978>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_images_v1, train_labels_v1, epochs=5, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 111us/step\n",
      "test_loss: 0.028, test_acc: 0.992\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images_v1, test_labels_v1)\n",
    "print(\"test_loss: %s, test_acc: %s\" %(np.round(test_loss,3), np.round(test_acc,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "41436783a21b52b878054a247ca0d3a389bf34f81c5598d0ecf92e6715f4dd38"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
